# üîç Guia de An√°lise de Erros e Compara√ß√£o de Modelos

## üìã Vis√£o Geral

Este guia explica como usar os scripts de an√°lise de erros e compara√ß√£o de modelos do DATALID 3.0.

---

## üîç Error Analysis (error_analysis.py)

### Objetivo
Analisa erros de predi√ß√£o do modelo, incluindo:
- **False Positives (FP)**: Detec√ß√µes incorretas
- **False Negatives (FN)**: Objetos n√£o detectados
- **Misclassifications**: Classe predita incorreta
- **True Positives (TP)**: Detec√ß√µes corretas
- **M√©tricas por classe**: Precision, Recall, F1, IoU m√©dio

### Uso B√°sico

#### Via Makefile (Recomendado)
```bash
# An√°lise no split de valida√ß√£o
make analyze-errors MODEL=experiments/nano-seg-10e/weights/best.pt DATA=data/processed/v1_segment

# Com par√¢metros customizados
make analyze-errors MODEL=path/to/model.pt DATA=path/to/dataset
```

#### Via Script Direto
```bash
# An√°lise b√°sica
python scripts/error_analysis.py \
    --model experiments/nano-seg-10e/weights/best.pt \
    --data data/processed/v1_segment

# An√°lise customizada
python scripts/error_analysis.py \
    --model experiments/nano-seg-10e/weights/best.pt \
    --data data/processed/v1_segment \
    --split val \
    --conf-threshold 0.5 \
    --iou-threshold 0.5 \
    --max-images 100 \
    --output-dir outputs/error_analysis/my_analysis
```

### Par√¢metros

| Par√¢metro | Descri√ß√£o | Padr√£o |
|-----------|-----------|--------|
| `--model` | Caminho do modelo (.pt) | **Obrigat√≥rio** |
| `--data` | Caminho do dataset (pasta com data.yaml) | **Obrigat√≥rio** |
| `--split` | Split para an√°lise (train/val/test) | `val` |
| `--conf-threshold` | Threshold de confidence | `0.25` |
| `--iou-threshold` | Threshold IoU para matching | `0.5` |
| `--max-images` | N√∫mero m√°ximo de imagens | Todas |
| `--no-visualizations` | N√£o salvar visualiza√ß√µes | `False` |
| `--output-dir` | Diret√≥rio de sa√≠da customizado | `outputs/error_analysis/{model_name}` |

### Sa√≠das

O script gera a seguinte estrutura de sa√≠da:

```
outputs/error_analysis/{model_name}/
‚îú‚îÄ‚îÄ error_analysis.json          # Estat√≠sticas completas
‚îú‚îÄ‚îÄ class_metrics.csv            # M√©tricas por classe
‚îú‚îÄ‚îÄ visualizations/
‚îÇ   ‚îú‚îÄ‚îÄ error_types.png          # Distribui√ß√£o de tipos de erro
‚îÇ   ‚îú‚îÄ‚îÄ class_metrics.png        # M√©tricas por classe (4 gr√°ficos)
‚îÇ   ‚îú‚îÄ‚îÄ iou_distribution.png     # Distribui√ß√£o de IoU scores
‚îÇ   ‚îî‚îÄ‚îÄ misclassification_matrix.png  # Matriz de confus√£o
‚îî‚îÄ‚îÄ errors/
    ‚îú‚îÄ‚îÄ image1_errors.jpg        # Imagens com erros anotados
    ‚îú‚îÄ‚îÄ image2_errors.jpg
    ‚îî‚îÄ‚îÄ ...
```

### Interpreta√ß√£o dos Resultados

#### M√©tricas Globais
- **Precision**: Quanto das detec√ß√µes est√£o corretas
- **Recall**: Quanto dos objetos foram detectados
- **F1 Score**: M√©dia harm√¥nica de precision e recall
- **Average IoU**: Qualidade das localiza√ß√µes

#### Tipos de Erro
- **False Positives**: Modelo detecta algo que n√£o existe
  - *Causas*: Background confuso, padr√µes similares
  - *Solu√ß√£o*: Mais dados, augmentation, ajustar threshold
  
- **False Negatives**: Modelo n√£o detecta objetos existentes
  - *Causas*: Objetos pequenos, oclus√£o, baixa qualidade
  - *Solu√ß√£o*: Aumentar recall, treinar mais epochs
  
- **Misclassifications**: Classe errada
  - *Causas*: Classes visualmente similares
  - *Solu√ß√£o*: Mais exemplos, feature engineering

#### Visualiza√ß√µes com Erros

Nas imagens salvas em `errors/`:
- üü¢ **Verde**: Ground Truth (correto)
- üü† **Laranja**: False Positive
- üü£ **Roxo**: Misclassification
- ‚ö™ **Aus√™ncia de box**: False Negative

---

## üìä Model Comparison (compare_models.py)

### Objetivo
Compara m√∫ltiplos modelos treinados e gera relat√≥rios com:
- M√©tricas de performance (mAP, precision, recall)
- Configura√ß√µes de treinamento
- Tempo de treinamento
- Tamanho dos modelos
- Rankings e visualiza√ß√µes comparativas

### Uso B√°sico

#### Via Makefile (Recomendado)
```bash
# Comparar todos os modelos no diret√≥rio experiments
make compare-models

# Comparar modelos espec√≠ficos
make compare-models MODELS="nano-seg-10e small-seg-10e medium-seg-10e"
```

#### Via Script Direto
```bash
# Comparar todos os modelos
python scripts/compare_models.py

# Comparar modelos espec√≠ficos
python scripts/compare_models.py \
    --models nano-seg-10e small-seg-10e medium-seg-10e

# Filtrar por padr√£o
python scripts/compare_models.py \
    --pattern "*-seg-*"

# Customizar output
python scripts/compare_models.py \
    --output-dir outputs/my_comparison \
    --rank-by map50_95
```

### Par√¢metros

| Par√¢metro | Descri√ß√£o | Padr√£o |
|-----------|-----------|--------|
| `--experiments-dir` | Diret√≥rio dos experimentos | `experiments` |
| `--models` | Nomes espec√≠ficos de modelos | Todos |
| `--pattern` | Padr√£o glob para filtrar | Nenhum |
| `--output-dir` | Diret√≥rio de sa√≠da | `outputs/model_comparison` |
| `--no-visualizations` | N√£o gerar gr√°ficos | `False` |
| `--rank-by` | M√©trica para ranking | `map50` |

### Sa√≠das

O script gera a seguinte estrutura:

```
outputs/model_comparison/
‚îú‚îÄ‚îÄ comparison_report.md         # Relat√≥rio completo em Markdown
‚îú‚îÄ‚îÄ model_comparison.csv         # Tabela comparativa (Excel)
‚îú‚îÄ‚îÄ model_comparison.json        # Dados estruturados
‚îî‚îÄ‚îÄ visualizations/
    ‚îú‚îÄ‚îÄ map50_comparison.png           # Barras comparando mAP@0.5
    ‚îú‚îÄ‚îÄ map50_95_comparison.png        # Barras comparando mAP@0.5:0.95
    ‚îú‚îÄ‚îÄ precision_recall.png           # Scatter plot P vs R
    ‚îú‚îÄ‚îÄ metrics_radar.png              # Radar chart multi-m√©trica
    ‚îú‚îÄ‚îÄ efficiency.png                 # Tempo vs Performance
    ‚îú‚îÄ‚îÄ size_vs_performance.png        # Tamanho vs Performance
    ‚îî‚îÄ‚îÄ metrics_heatmap.png            # Heatmap normalizado
```

### Visualiza√ß√µes

#### 1. mAP Comparison
Gr√°fico de barras mostrando mAP@0.5 e mAP@0.5:0.95 de cada modelo.
- **Uso**: Identificar modelo com melhor performance geral

#### 2. Precision vs Recall
Scatter plot mostrando trade-off entre precision e recall.
- **Uso**: Escolher modelo baseado em prioridade (FP vs FN)
- **Ideal**: Canto superior direito (alta P e R)

#### 3. Metrics Radar
Radar chart com m√∫ltiplas m√©tricas normalizadas.
- **Uso**: Vis√£o hol√≠stica de cada modelo
- **Ideal**: √Årea maior = melhor performance geral

#### 4. Efficiency Plot
Tempo de treinamento vs performance.
- **Uso**: Identificar melhor custo-benef√≠cio
- **Ideal**: Canto superior esquerdo (r√°pido e bom)

#### 5. Size vs Performance
Tamanho do modelo vs performance.
- **Uso**: Escolher para deployment (edge devices)
- **Ideal**: Canto superior esquerdo (pequeno e bom)

#### 6. Metrics Heatmap
Heatmap normalizado de todas as m√©tricas.
- **Uso**: Compara√ß√£o detalhada multi-dimensional
- **Verde**: Melhor, **Vermelho**: Pior

---

## üéØ Casos de Uso Pr√°ticos

### Caso 1: An√°lise P√≥s-Treinamento

Ap√≥s treinar um modelo, analise seus erros:

```bash
# 1. Treinar modelo
make train-final-nano

# 2. Analisar erros
make analyze-errors \
    MODEL=experiments/final-nano-segment/weights/best.pt \
    DATA=data/processed/v1_segment

# 3. Revisar resultados
# - Abrir outputs/error_analysis/final-nano-segment/visualizations/
# - Verificar class_metrics.csv
# - Analisar imagens com erros em errors/
```

### Caso 2: Compara√ß√£o de Experimentos

Compare diferentes configura√ß√µes de treinamento:

```bash
# 1. Treinar m√∫ltiplos modelos
make train-final-nano
make train-final-small
make train-final-medium

# 2. Comparar
make compare-models

# 3. Revisar relat√≥rio
# - Abrir outputs/model_comparison/comparison_report.md
# - Verificar visualizations/
```

### Caso 3: Escolha de Modelo para Produ√ß√£o

```bash
# 1. Comparar todos os modelos
python scripts/compare_models.py --rank-by map50_95

# 2. Analisar trade-offs:
# - Size vs Performance (para edge devices)
# - Efficiency (para custo de treinamento)
# - Precision vs Recall (para aplica√ß√£o espec√≠fica)

# 3. An√°lise detalhada do melhor candidato
python scripts/error_analysis.py \
    --model experiments/chosen-model/weights/best.pt \
    --data data/processed/v1_segment \
    --conf-threshold 0.5
```

### Caso 4: Debug de Classes Problem√°ticas

```bash
# 1. An√°lise geral
make analyze-errors MODEL=model.pt DATA=dataset

# 2. Revisar class_metrics.csv para identificar classes ruins

# 3. Verificar imagens com erros da classe espec√≠fica em errors/

# 4. A√ß√µes:
# - Aumentar dados da classe problem√°tica
# - Revisar anota√ß√µes
# - Ajustar data augmentation
```

---

## üîß Integra√ß√£o com Workflow

### Pipeline Completo de Experimenta√ß√£o

```bash
# 1. Processar dados
make quick-process

# 2. Treinar modelos
make train-compare-all

# 3. Comparar resultados
make compare-models

# 4. Analisar melhor modelo
make analyze-errors \
    MODEL=experiments/best-model/weights/best.pt \
    DATA=data/processed/v1_segment

# 5. Ajustar e retreinar se necess√°rio
```

---

## üìä M√©tricas Importantes

### Para Classifica√ß√£o/Detec√ß√£o

| M√©trica | O que mede | Quando usar |
|---------|------------|-------------|
| **mAP@0.5** | Performance geral (IoU ‚â• 0.5) | Compara√ß√£o padr√£o COCO |
| **mAP@0.5:0.95** | Performance rigorosa (m√∫ltiplos IoUs) | Avalia√ß√£o completa |
| **Precision** | Acur√°cia das detec√ß√µes | Minimizar FP |
| **Recall** | Completude das detec√ß√µes | Minimizar FN |
| **F1 Score** | Balan√ßo P e R | M√©trica √∫nica balanceada |
| **IoU** | Qualidade da localiza√ß√£o | Ajuste fino de boxes |

### Para Escolha de Modelo

| Objetivo | M√©trica Prim√°ria | M√©trica Secund√°ria |
|----------|------------------|-------------------|
| **Melhor Performance** | mAP@0.5:0.95 | Precision |
| **Produ√ß√£o (Edge)** | mAP@0.5 + Model Size | Inference Time |
| **Custo de Treinamento** | mAP@0.5 + Training Time | Epochs |
| **Aplica√ß√£o Cr√≠tica (FN caros)** | Recall | mAP@0.5 |
| **Aplica√ß√£o Cr√≠tica (FP caros)** | Precision | mAP@0.5 |

---

## üí° Dicas e Boas Pr√°ticas

### An√°lise de Erros

1. **Sempre analise no split de valida√ß√£o**, n√£o em train
2. **Ajuste conf-threshold** baseado na aplica√ß√£o:
   - Alta precision: threshold alto (0.5-0.7)
   - Alta recall: threshold baixo (0.1-0.3)
3. **Salve visualiza√ß√µes** para apresenta√ß√µes e debug
4. **Foque em classes com baixo F1** para melhorias

### Compara√ß√£o de Modelos

1. **Compare modelos similares**: Mesmos dados e splits
2. **Considere m√∫ltiplas m√©tricas**: N√£o s√≥ mAP
3. **Avalie trade-offs**: Performance vs Tamanho vs Tempo
4. **Use --pattern** para compara√ß√µes espec√≠ficas
5. **Salve relat√≥rios** para refer√™ncia futura

### Interpreta√ß√£o

1. **FP alto**: Dados de background, augmentation, threshold
2. **FN alto**: Objetos dif√≠ceis, mais epochs, arquitetura maior
3. **Misclassifications**: Classes similares, mais dados espec√≠ficos
4. **Baixo IoU**: Qualidade das anota√ß√µes, ajuste de boxes

---

## üö® Troubleshooting

### Erro: "Modelo n√£o encontrado"
```bash
# Verificar path correto
ls experiments/*/weights/best.pt

# Usar path absoluto se necess√°rio
```

### Erro: "Dataset n√£o encontrado"
```bash
# Verificar estrutura
ls data/processed/v1_segment/
# Deve conter: data.yaml, train/, val/, test/
```

### Erro: "M√©tricas n√£o dispon√≠veis"
```bash
# Verificar se treinamento foi conclu√≠do
ls experiments/model-name/results.csv
```

### Compara√ß√£o sem visualiza√ß√µes
```bash
# Instalar depend√™ncias
pip install matplotlib seaborn pandas
```

---

## üìö Refer√™ncias

- [YOLO Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/)
- [mAP Calculation](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)
- [Precision vs Recall Trade-off](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)

---

**Criado por**: DATALID 3.0  
**Vers√£o**: 1.0  
**Data**: 2025
