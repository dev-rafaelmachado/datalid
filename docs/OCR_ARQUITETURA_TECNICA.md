# ğŸ›ï¸ Arquitetura TÃ©cnica Profunda: OCR no Datalid 3.0

## I. Arquitetura em Camadas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CAMADA DE APRESENTAÃ‡ÃƒO                     â”‚
â”‚                    (Scripts & AplicaÃ§Ãµes Finais)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ scripts/ocr/benchmark_ocrs.py                                  â”‚
â”‚  â€¢ scripts/ocr/benchmark_parseq_enhanced.py                       â”‚
â”‚  â€¢ Makefile (make ocr-compare, make ocr-annotate, etc.)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CAMADA DE ORQUESTRAÃ‡ÃƒO                          â”‚
â”‚                    (OCREvaluator + ConfiguraÃ§Ã£o)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  src/ocr/evaluator.py                                             â”‚
â”‚  â”œâ”€ OCREvaluator (compara mÃºltiplos engines)                      â”‚
â”‚  â”œâ”€ load_ocr_config() â†’ Carrega YAML                              â”‚
â”‚  â”œâ”€ load_preprocessing_config() â†’ Carrega YAML                    â”‚
â”‚  â””â”€ merge_configs() â†’ Sobrescreve configs                         â”‚
â”‚                                                                    â”‚
â”‚  config/ocr/*.yaml          config/preprocessing/*.yaml            â”‚
â”‚  â”œâ”€ default.yaml            â”œâ”€ minimal.yaml                        â”‚
â”‚  â”œâ”€ tesseract.yaml          â”œâ”€ medium.yaml                         â”‚
â”‚  â”œâ”€ easyocr.yaml            â”œâ”€ heavy.yaml                          â”‚
â”‚  â”œâ”€ paddleocr.yaml          â””â”€ ppro-*.yaml (specializados)        â”‚
â”‚  â”œâ”€ parseq.yaml                                                    â”‚
â”‚  â””â”€ enhanced_parseq.yaml                                           â”‚
â”‚                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   â”‚                   â”‚
     â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRÃ‰-PROCES.    â”‚ â”‚   ENGINES   â”‚ â”‚  PÃ“S-PROCESSAMENTO       â”‚
â”‚                â”‚ â”‚      OCR    â”‚ â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                â”‚ â”‚             â”‚ â”‚                          â”‚
â”‚ ImagePreproc   â”‚ â”‚  5 Engines  â”‚ â”‚ ContextualPostprocessor  â”‚
â”‚ .process()     â”‚ â”‚             â”‚ â”‚ .process()               â”‚
â”‚                â”‚ â”‚ [Base]      â”‚ â”‚                          â”‚
â”‚ â€¢ resize       â”‚ â”‚ â”œâ”€ Tesseractâ”‚ â”‚ DateParser.parse()       â”‚
â”‚ â€¢ grayscale    â”‚ â”‚ â”œâ”€ EasyOCR  â”‚ â”‚                          â”‚
â”‚ â€¢ shadow_rm    â”‚ â”‚ â”œâ”€ PaddleOCRâ”‚ â”‚ FunÃ§Ãµes principais:      â”‚
â”‚ â€¢ deskew       â”‚ â”‚ â”œâ”€ TrOCR    â”‚ â”‚ â€¢ uppercase              â”‚
â”‚ â€¢ clahe        â”‚ â”‚ â””â”€ PARSeq   â”‚ â”‚ â€¢ remove_symbols         â”‚
â”‚ â€¢ morphology   â”‚ â”‚             â”‚ â”‚ â€¢ ambiguity_mapping      â”‚
â”‚ â€¢ sharpen      â”‚ â”‚ [Enhanced]  â”‚ â”‚ â€¢ fix_formats            â”‚
â”‚ â€¢ denoise      â”‚ â”‚ â””â”€ Enhanced â”‚ â”‚ â€¢ fuzzy_match            â”‚
â”‚ â€¢ padding      â”‚ â”‚   PARSeq    â”‚ â”‚ â€¢ cleanup                â”‚
â”‚                â”‚ â”‚             â”‚ â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## II. Camada de Engines - Arquitetura Detalhada

### A. Hierarquia de Classes

```
OCREngineBase (Abstract)
    â”‚
    â”œâ”€â”€ TesseractEngine
    â”œâ”€â”€ EasyOCREngine
    â”œâ”€â”€ PaddleOCREngine
    â”œâ”€â”€ TrOCREngine
    â”œâ”€â”€ PARSeqEngine
    â”‚   â”‚
    â”‚   â””â”€â”€ EnhancedPARSeqEngine â† Estende com features avanÃ§adas
    â”‚       â”œâ”€â”€ uses: LineDetector
    â”‚       â”œâ”€â”€ uses: GeometricNormalizer
    â”‚       â”œâ”€â”€ uses: PhotometricNormalizer
    â”‚       â””â”€â”€ uses: ContextualPostprocessor
```

### B. Interface Base (OCREngineBase)

```python
class OCREngineBase(ABC):
    
    # Interface que TODOS os engines precisam implementar
    @abstractmethod
    def initialize(self) -> None:
        """Carrega o modelo"""
    
    @abstractmethod
    def extract_text(self, image) -> Tuple[str, float]:
        """OCR na imagem
        Returns: (texto, confianÃ§a)
        """
    
    @abstractmethod
    def get_name(self) -> str:
        """Nome do engine"""
    
    # MÃ©todos helper (implementados)
    def validate_image(self, image) -> bool:
        """Valida se imagem Ã© adequada"""
    
    def postprocess(self, text: str) -> str:
        """Limpeza bÃ¡sica"""
```

### C. Fluxo EspecÃ­fico - PaddleOCR (Exemplo Simples)

```
Input: np.ndarray (BGR)
  â”‚
  â”œâ”€ Initialize()
  â”‚  â””â”€ from paddleocr import PaddleOCR
  â”‚     engine = PaddleOCR(lang='pt', use_angle_cls=True)
  â”‚
  â”œâ”€ Extract Text()
  â”‚  â”œâ”€ results = engine.ocr(image)
  â”‚  â”‚  â””â”€ Retorna: [[bbox_list], [text_list], [conf_list]]
  â”‚  â”‚
  â”‚  â”œâ”€ Parse resultado (tratar compatibilidade de versÃµes)
  â”‚  â”‚  â”œâ”€ Extract: texts, confidences
  â”‚  â”‚  â”œâ”€ Filter: conf >= threshold
  â”‚  â”‚  â””â”€ Combine: ' '.join(texts)
  â”‚  â”‚
  â”‚  â”œâ”€ Postprocess()
  â”‚  â”‚  â””â”€ Remove espaÃ§os extras, strip
  â”‚  â”‚
  â”‚  â””â”€ Return: (texto, avg_confidence)
  â”‚
  â””â”€ Output: ("LOTE 202", 0.88)
```

### D. Fluxo Complexo - Enhanced PARSeq (Seu Destaque)

```
Input: np.ndarray (imagem multi-linha)
  â”‚
  â”œâ”€ Initialize()
  â”‚  â””â”€ Carregar modelo PARSeq do torch.hub
  â”‚     device: 'cuda' ou 'cpu'
  â”‚
  â”œâ”€ Extract Text()
  â”‚  â”‚
  â”‚  â”œâ”€ OPÃ‡ÃƒO 1: Sem Line Detection (modo simples)
  â”‚  â”‚  â””â”€ NormalizaÃ§Ã£o fotomÃ©trica â†’ PARSeq OCR â†’ Output
  â”‚  â”‚
  â”‚  â””â”€ OPÃ‡ÃƒO 2: Com Line Detection (modo avanÃ§ado)
  â”‚     â”‚
  â”‚     â”œâ”€ [1] LineDetector.detect_lines()
  â”‚     â”‚  â”‚
  â”‚     â”‚  â”œâ”€ Converter para grayscale
  â”‚     â”‚  â”œâ”€ Detectar rotaÃ§Ã£o (Hough Transform)
  â”‚     â”‚  â”‚  â””â”€ Se rotaÃ§Ã£o > max_angle â†’ corrigir
  â”‚     â”‚  â”‚
  â”‚     â”‚  â”œâ”€ Escolher mÃ©todo:
  â”‚     â”‚  â”‚  â”œâ”€ Projection Profile (histograma)
  â”‚     â”‚  â”‚  â”‚  â””â”€ Proj_vert[y] = sum(pixel[y,:])
  â”‚     â”‚  â”‚  â”‚     Detectar picos = linhas
  â”‚     â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â”œâ”€ Clustering (DBSCAN)
  â”‚     â”‚  â”‚  â”‚  â””â”€ CC = contornos detectados
  â”‚     â”‚  â”‚  â”‚     Clusterizar CC por Y
  â”‚     â”‚  â”‚  â”‚     eps=15, min_samples=1
  â”‚     â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â”œâ”€ Morphological
  â”‚     â”‚  â”‚  â”‚  â””â”€ Kernel dilat horiz
  â”‚     â”‚  â”‚  â”‚     Agrupar em linhas
  â”‚     â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â””â”€ Hybrid (melhor)
  â”‚     â”‚  â”‚     â””â”€ Tentar todos, usar melhor
  â”‚     â”‚  â”‚
  â”‚     â”‚  â”œâ”€ Filtrar linhas pequenas
  â”‚     â”‚  â”œâ”€ Ordenar top-to-bottom
  â”‚     â”‚  â””â”€ Return: [(x,y,w,h), ...]
  â”‚     â”‚
  â”‚     â”œâ”€ [2] LineDetector.split_lines()
  â”‚     â”‚  â””â”€ For cada bbox: crop[y:y+h, x:x+w]
  â”‚     â”‚     Return: [line1, line2, ...]
  â”‚     â”‚
  â”‚     â”œâ”€ [3] Para CADA LINHA:
  â”‚     â”‚  â”‚
  â”‚     â”‚  â”œâ”€ GeometricNormalizer.normalize()
  â”‚     â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â”œâ”€ deskew()
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Edge detection (Canny)
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Hough lines
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Calcular Ã¢ngulo mediano
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Rotacionar com getRotationMatrix2D()
  â”‚     â”‚  â”‚  â”‚  â””â”€ Limitar Ã¢ngulo max Â±10Â°
  â”‚     â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â”œâ”€ perspective_warp()
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Binarizar
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Encontrar contorno principal
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ minAreaRect()
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Sanity checks:
  â”‚     â”‚  â”‚  â”‚  â”‚  â”œâ”€ Ãrea < 30% original â†’ skip
  â”‚     â”‚  â”‚  â”‚  â”‚  â”œâ”€ Aspect > 20 â†’ skip
  â”‚     â”‚  â”‚  â”‚  â”‚  â”œâ”€ Ã‚ngulo > 15Â° â†’ skip
  â”‚     â”‚  â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â”‚  â””â”€ perspectiveTransform()
  â”‚     â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â””â”€ resize(target_height)
  â”‚     â”‚  â”‚     â””â”€ Keep aspect ratio
  â”‚     â”‚  â”‚
  â”‚     â”‚  â”œâ”€ IF ENSEMBLE:
  â”‚     â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â”œâ”€ PhotometricNormalizer.generate_variants()
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Variant 1: denoise apenas
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Variant 2: denoise + CLAHE
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Variant 3: denoise + sharpen
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Variant 4: denoise + CLAHE + sharpen
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Variant 5: shadow_removal
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Variant 6: shadow_removal + CLAHE
  â”‚     â”‚  â”‚  â”‚  â””â”€ Variant 7: shadow_removal + sharpen
  â”‚     â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â”œâ”€ FOR cada variante:
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ PARSeq inference
  â”‚     â”‚  â”‚  â”‚  â”œâ”€ Get: (texto, logprobs)
  â”‚     â”‚  â”‚  â”‚  â””â”€ Store: {texto, conf}
  â”‚     â”‚  â”‚  â”‚
  â”‚     â”‚  â”‚  â””â”€ Reranking:
  â”‚     â”‚  â”‚     FOR each result:
  â”‚     â”‚  â”‚     â”œâ”€ conf = exp(logprob).mean()
  â”‚     â”‚  â”‚     â”œâ”€ penalty = 0
  â”‚     â”‚  â”‚     â”œâ”€ IF len(texto) < 3: penalty -= 0.30
  â”‚     â”‚  â”‚     â”œâ”€ IF sÃ­mbolos > 30%: penalty -= 0.20
  â”‚     â”‚  â”‚     â”œâ”€ IF espaÃ§os > 20%: penalty -= 0.15
  â”‚     â”‚  â”‚     â”œâ”€ score = 0.5*conf + penalty
  â”‚     â”‚  â”‚     â””â”€ SELECT argmax(score)
  â”‚     â”‚  â”‚
  â”‚     â”‚  â””â”€ ELSE (sem ensemble):
  â”‚     â”‚     â”œâ”€ PhotometricNormalizer.normalize() [single]
  â”‚     â”‚     â”œâ”€ PARSeq inference
  â”‚     â”‚     â””â”€ Return: (texto, conf)
  â”‚     â”‚
  â”‚     â””â”€ [4] Combinar linhas
  â”‚        â””â”€ result = join([line1, line2, ...], '\n')
  â”‚
  â”œâ”€ ContextualPostprocessor.process()
  â”‚  â”‚
  â”‚  â”œâ”€ Uppercase
  â”‚  â”‚  â””â”€ texto.upper()
  â”‚  â”‚
  â”‚  â”œâ”€ Remove symbols
  â”‚  â”‚  â””â”€ re.sub(r'[^A-Za-z0-9\s/\-.:]+', '', texto)
  â”‚  â”‚
  â”‚  â”œâ”€ Ambiguity mapping
  â”‚  â”‚  â”œâ”€ Detectar contexto (anterior/posterior)
  â”‚  â”‚  â”œâ”€ SE contexto numÃ©rico:
  â”‚  â”‚  â”‚  â”œâ”€ O â†’ 0, I â†’ 1, S â†’ 5, etc.
  â”‚  â”‚  â””â”€ SE contexto alfabÃ©tico:
  â”‚  â”‚     â”œâ”€ 0 â†’ O, 1 â†’ I, etc.
  â”‚  â”‚
  â”‚  â”œâ”€ Fuzzy matching (Levenshtein)
  â”‚  â”‚  FOR each word in texto:
  â”‚  â”‚  â”œâ”€ FOR each known_word:
  â”‚  â”‚  â”‚  â”œâ”€ dist = levenshtein(word, known_word)
  â”‚  â”‚  â”‚  â”œâ”€ IF dist <= threshold:
  â”‚  â”‚  â”‚  â”‚  â””â”€ word = known_word (correÃ§Ã£o)
  â”‚  â”‚  â”‚  â”‚
  â”‚  â”‚  â””â”€ Replace word no texto
  â”‚  â”‚
  â”‚  â”œâ”€ Fix formats
  â”‚  â”‚  â”œâ”€ IF match(r'LOT[EO]'):
  â”‚  â”‚  â”‚  â””â”€ Normalize para 'LOTE'
  â”‚  â”‚  â””â”€ etc.
  â”‚  â”‚
  â”‚  â””â”€ Final cleanup
  â”‚     â””â”€ ' '.join(texto.split())
  â”‚
  â””â”€ Output: ("VAL:18/06/2026\nLOTE:2506185776", 0.94)
```

---

## III. Detalhes das NormalizaÃ§Ãµes

### A. NormalizaÃ§Ã£o GeomÃ©trica (GeometricNormalizer)

#### 1. Deskew Algorithm
```
Input: image_with_skew

â”œâ”€ Edge Detection
â”‚  â””â”€ gray = cv2.cvtColor(BGR â†’ GRAY)
â”‚     edges = cv2.Canny(gray, 50, 150)
â”‚
â”œâ”€ Hough Lines
â”‚  â””â”€ lines = cv2.HoughLines(edges, rho=1, theta=Ï€/180, threshold=100)
â”‚
â”œâ”€ Angle Extraction
â”‚  FOR each (rho, theta) in lines:
â”‚     angle_deg = degrees(theta) - 90
â”‚     IF |angle| < 45:
â”‚        angles.append(angle_deg)
â”‚
â”œâ”€ Robust Estimation
â”‚  median_angle = median(angles)
â”‚
â”œâ”€ Angle Clipping
â”‚  IF |median_angle| > max_angle (default 10Â°):
â”‚     median_angle = clip(median_angle, -max_angle, max_angle)
â”‚
â”œâ”€ Calculate Rotation Matrix
â”‚  M = cv2.getRotationMatrix2D(center, angle, scale=1.0)
â”‚
â”œâ”€ Adjust Translation
â”‚  M[0,2] += (new_width/2 - center_x)
â”‚  M[1,2] += (new_height/2 - center_y)
â”‚
â””â”€ Apply Rotation
   rotated = cv2.warpAffine(image, M, (new_w, new_h))
   BORDER_MODE: REPLICATE (nÃ£o deixar preto)

Output: rotation_corrected_image
```

#### 2. Perspective Warp Algorithm
```
Input: image_with_perspective

â”œâ”€ Preprocessing
â”‚  gray = cvtColor(BGR â†’ GRAY)
â”‚  binary = cv2.threshold(gray, BINARY_INV + OTSU)
â”‚
â”œâ”€ Contour Detection
â”‚  contours = cv2.findContours(binary)
â”‚  main_contour = argmax(area(contours))
â”‚
â”œâ”€ Sanity Checks
â”‚  area = cv2.contourArea(main_contour)
â”‚  image_area = width Ã— height
â”‚  
â”‚  IF area < 0.3 * image_area:
â”‚     RETURN original (contorno muito pequeno)
â”‚
â”‚  rect = cv2.minAreaRect(main_contour)
â”‚  (center, (w, h), angle) = rect
â”‚  aspect = max(w, h) / min(w, h)
â”‚  
â”‚  IF aspect > 20:
â”‚     RETURN original (aspecto extremo)
â”‚  
â”‚  IF |angle| > 15Â°:
â”‚     RETURN original (Ã¢ngulo muito grande)
â”‚
â”œâ”€ Calculate Perspective Transform
â”‚  pts_src = cv2.boxPoints(rect)
â”‚  pts_dst = [[0,0], [width,0], [width,height], [0,height]]
â”‚  M = cv2.getPerspectiveTransform(pts_src, pts_dst)
â”‚
â””â”€ Apply Transform
   warped = cv2.warpPerspective(image, M, (w, h))

Output: perspective_corrected_image
```

### B. NormalizaÃ§Ã£o FotomÃ©trica (PhotometricNormalizer)

```
Input: line_image (single line, geometrically normalized)

â”œâ”€ Denoise
â”‚  â”œâ”€ Method 1: Median Filter
â”‚  â”‚  â””â”€ cv2.medianBlur(image, ksize=3)
â”‚  â”‚
â”‚  â””â”€ Method 2: Bilateral Filter
â”‚     â””â”€ cv2.bilateralFilter(image, d=7, sigma_color=75, sigma_space=75)
â”‚
â”œâ”€ Shadow Removal
â”‚  â”œâ”€ Background = cv2.blur(image, ksize=(21,21))
â”‚  â””â”€ Result = image - 0.5 * background (background subtraction)
â”‚
â”œâ”€ CLAHE (Contrast Limited Adaptive Histogram Equalization)
â”‚  â”œâ”€ Create: clahe = cv2.createCLAHE(clip_limit=1.5, tile_grid=(8,8))
â”‚  â””â”€ Apply: result = clahe.apply(image)
â”‚
â”œâ”€ Morphological Operations (opcional)
â”‚  â”œâ”€ kernel = cv2.getStructuringElement(MORPH_ELLIPSE, (3,3))
â”‚  â””â”€ result = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)
â”‚
â”œâ”€ Sharpen
â”‚  â”œâ”€ kernel = [[0, -1, 0],
â”‚  â”‚             [-1, 5, -1],
â”‚  â”‚             [0, -1, 0]]
â”‚  â””â”€ sharpened = cv2.filter2D(image, -1, kernel) Ã— strength
â”‚     final = image + sharpened
â”‚
â””â”€ Output: normalized_line_image
```

### C. GeraÃ§Ã£o de Variantes (Ensemble)

```
Input: single_line_image (geometrically normalized)

â”œâ”€ Variant 1: Baseline (denoise only)
â”‚  â””â”€ PhotometricNormalizer.apply(method='denoise_only')
â”‚
â”œâ”€ Variant 2: Denoise + CLAHE
â”‚  â””â”€ denoise â†’ CLAHE
â”‚
â”œâ”€ Variant 3: Denoise + Sharpen
â”‚  â””â”€ denoise â†’ sharpen
â”‚
â”œâ”€ Variant 4: Denoise + CLAHE + Sharpen
â”‚  â””â”€ denoise â†’ CLAHE â†’ sharpen
â”‚
â”œâ”€ Variant 5: Shadow Removal
â”‚  â””â”€ shadow_removal
â”‚
â”œâ”€ Variant 6: Shadow Removal + CLAHE
â”‚  â””â”€ shadow_removal â†’ CLAHE
â”‚
â””â”€ Variant 7: Shadow Removal + Sharpen
   â””â”€ shadow_removal â†’ sharpen

Output: [variant_1, variant_2, ..., variant_7]
        (7 versÃµes diferentes da mesma imagem)
```

---

## IV. DetecÃ§Ã£o de Linhas (LineDetector)

### A. Projection Profile Method

```
Input: binary_image (background/text invertido)

â”œâ”€ Calculate Horizontal Projection
â”‚  projection[y] = sum(pixel[y, :])
â”‚
â”œâ”€ Smooth Projection
â”‚  kernel_size = max(3, min_line_height / 3)
â”‚  smooth_proj = convolve(projection, ones(kernel_size))
â”‚
â”œâ”€ Detect Peaks (linhas)
â”‚  threshold = mean(projection) * 0.3
â”‚  in_line[y] = (smooth_proj[y] > threshold)
â”‚
â”œâ”€ Find Continuous Regions
â”‚  FOR y in range(height):
â”‚     IF in_line[y] and not in_current_line:
â”‚        start_y = y
â”‚     ELIF not in_line[y] and in_current_line:
â”‚        end_y = y
â”‚        lines.append((0, start_y, width, end_y-start_y))
â”‚
â””â”€ Output: [(x, y, w, h), ...]
```

### B. DBSCAN Clustering Method

```
Input: image_with_text

â”œâ”€ Find Connected Components
â”‚  binary = threshold(image)
â”‚  contours = findContours(binary)
â”‚
â”œâ”€ Extract Centers
â”‚  FOR each contour:
â”‚     moments = moments(contour)
â”‚     cx = moments['m10'] / moments['m00']
â”‚     cy = moments['m01'] / moments['m00']
â”‚     centers.append((cy, cx))  # NOTE: sorted by Y
â”‚
â”œâ”€ DBSCAN Clustering
â”‚  â”œâ”€ eps = 15 (max distance between points in cluster)
â”‚  â”œâ”€ min_samples = 1
â”‚  â””â”€ clusters = DBSCAN(centers, eps=eps, min_samples=min_samples)
â”‚
â”œâ”€ Create Line Bboxes
â”‚  FOR each cluster:
â”‚     cluster_points = centers[cluster_indices]
â”‚     y_min = min(y for y, x in cluster_points)
â”‚     y_max = max(y for y, x in cluster_points)
â”‚     x_min = min(x for y, x in cluster_points)
â”‚     x_max = max(x for y, x in cluster_points)
â”‚     bbox = (x_min, y_min, x_max-x_min, y_max-y_min)
â”‚     lines.append(bbox)
â”‚
â””â”€ Output: [(x, y, w, h), ...] sorted by Y
```

### C. Morphological Method

```
Input: image_with_text

â”œâ”€ Create Horizontal Kernel
â”‚  kernel = getStructuringElement(MORPH_RECT, (morphology_kernel_width, 1))
â”‚
â”œâ”€ Dilate Horizontally
â”‚  â””â”€ dilated = morphologyEx(image, MORPH_CLOSE, kernel)
â”‚     (conecta pixels horizontalmente, separando linhas verticalmente)
â”‚
â”œâ”€ Find Contours
â”‚  contours = findContours(dilated)
â”‚
â”œâ”€ Create Line Bboxes
â”‚  FOR each contour:
â”‚     bbox = boundingRect(contour)
â”‚     x, y, w, h = bbox
â”‚     IF h >= min_line_height and w >= min_component_width:
â”‚        lines.append((x, y, w, h))
â”‚
â””â”€ Output: [(x, y, w, h), ...] sorted by Y
```

---

## V. InferÃªncia PARSeq - Detalhado

```
Input: single_line_image (normalized, 32Ã—128, grayscale/RGB)

â”œâ”€ Image Preprocessing
â”‚  â”œâ”€ PIL.Image.fromarray(image)
â”‚  â”œâ”€ transforms.Resize((32, 128), BICUBIC)
â”‚  â”œâ”€ transforms.ToTensor()
â”‚  â””â”€ transforms.Normalize(ImageNet_stats)
â”‚
â”œâ”€ Forward Pass (Backbone + Encoder)
â”‚  â”œâ”€ CNN Backbone: ResNet
â”‚  â”‚  â””â”€ features: CÃ—HÃ—W (C=2048 tÃ­pico)
â”‚  â”‚
â”‚  â””â”€ Transformer Encoder
â”‚     â”œâ”€ Positional encoding
â”‚     â”œâ”€ Self-attention layers (12 layers tÃ­pico)
â”‚     â””â”€ Output: contextualized_features
â”‚
â”œâ”€ Permutation Auto-Regression Decoding
â”‚  â”‚
â”‚  â”œâ”€ Initialize: [START] token
â”‚  â”‚
â”‚  â”œâ”€ FOR step in range(max_length):
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Query Available Positions
â”‚  â”‚  â”‚  positions_available = {all - predicted}
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Decode Step
â”‚  â”‚  â”‚  logits = decoder(contextualized_features, positions_available)
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€ Predict next token
â”‚  â”‚  â”‚  â”‚  next_token = argmax(logits)
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€ Predict next position
â”‚  â”‚  â”‚     next_pos = argmax(position_logits)
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Append to Sequence
â”‚  â”‚  â”‚  sequence.append(next_token)
â”‚  â”‚  â”‚  predicted_positions.append(next_pos)
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Check Stop
â”‚  â”‚     IF next_token == [END]:
â”‚  â”‚        BREAK
â”‚  â”‚
â”‚  â””â”€ Output: token_sequence, log_probabilities
â”‚
â”œâ”€ Post-processing
â”‚  â”œâ”€ Remove [START] and [END] tokens
â”‚  â”œâ”€ Decode tokens to characters
â”‚  â””â”€ confidence = exp(log_probs).mean()
â”‚
â””â”€ Output: (predicted_text, confidence_score)
```

---

## VI. Reranking Algorithm (Enhanced PARSeq)

```
Input: results_from_ensemble = [
    {'text': 'LOTE', 'logprob': [-0.1, -0.15, ...], 'conf': 0.92},
    {'text': 'L0TE', 'logprob': [-0.2, -0.25, ...], 'conf': 0.85},
    ...
]

FOR each result in results_from_ensemble:
    
    â”œâ”€ Base Confidence
    â”‚  conf = result['confidence']
    â”‚
    â”œâ”€ Initialize Penalties
    â”‚  penalty = 0.0
    â”‚
    â”œâ”€ Penalty 1: Text Length
    â”‚  IF len(result['text']) < 3:
    â”‚     penalty -= 0.30  # Penalize very short text
    â”‚
    â”œâ”€ Penalty 2: Symbol Count
    â”‚  symbol_ratio = count_symbols(result['text']) / len(result['text'])
    â”‚  IF symbol_ratio > 0.30:
    â”‚     penalty -= 0.20 * symbol_ratio
    â”‚
    â”œâ”€ Penalty 3: Space Count
    â”‚  space_ratio = count_spaces(result['text']) / len(result['text'])
    â”‚  IF space_ratio > 0.20:
    â”‚     penalty -= 0.15 * space_ratio
    â”‚
    â”œâ”€ Calculate Final Score
    â”‚  score = 0.5 * conf + penalty
    â”‚     (50% peso para confianÃ§a base, resto para penalidades)
    â”‚
    â””â”€ Store Score
       result['rerank_score'] = score

OUTPUT: best_result = argmax(rerank_score)
        Return: (best_result['text'], best_result['confidence'])
```

---

## VII. Postprocessamento - Detalhado

### A. Ambiguity Mapping Algorithm

```
Input: text = "L0TE 202"

FOR i, char in enumerate(text):
    
    â”œâ”€ Check Character Ambiguity
    â”‚  IF char in ['O', 'I', 'S', 'l', 'i', 'o', '1', '0', '5', '8', 'B', 'G', 'Z', 'T']:
    â”‚
    â”‚     â”œâ”€ Get Context
    â”‚     â”‚  prev_is_digit = (i > 0 and text[i-1].isdigit())
    â”‚     â”‚  next_is_digit = (i < len(text)-1 and text[i+1].isdigit())
    â”‚     â”‚
    â”‚     â”‚  prev_is_alpha = (i > 0 and text[i-1].isalpha())
    â”‚     â”‚  next_is_alpha = (i < len(text)-1 and text[i+1].isalpha())
    â”‚     â”‚
    â”‚     â””â”€ Determine Context Type
    â”‚        IF prev_is_digit or next_is_digit:
    â”‚           context = 'NUMERIC'
    â”‚        ELIF prev_is_alpha or next_is_alpha:
    â”‚           context = 'ALPHA'
    â”‚        ELSE:
    â”‚           context = 'MIXED'
    â”‚
    â”‚     â”œâ”€ Apply Mapping
    â”‚     â”‚  IF context == 'NUMERIC':
    â”‚     â”‚     â”œâ”€ 'O' â†’ '0'
    â”‚     â”‚     â”œâ”€ 'I' â†’ '1'
    â”‚     â”‚     â”œâ”€ 'l' â†’ '1'
    â”‚     â”‚     â”œâ”€ 'S' â†’ '5'
    â”‚     â”‚     â””â”€ 'B' â†’ '8'
    â”‚     â”‚
    â”‚     â””â”€ IF context == 'ALPHA':
    â”‚        â””â”€ 'I' â† '1' (if isolated)
    â”‚
    â””â”€ Output: mapped_char

Output: mapped_text
```

### B. Fuzzy Matching Algorithm

```
Input: text = "LOTE"
       known_words = ['LOT', 'LOTE', 'DATE', 'BATCH']

FOR each word in text.split():
    
    â”œâ”€ Find Best Match
    â”‚  best_match = None
    â”‚  best_distance = inf
    â”‚  
    â”‚  FOR each known_word in known_words:
    â”‚     distance = levenshtein_distance(word, known_word)
    â”‚     
    â”‚     IF distance < best_distance:
    â”‚        best_distance = distance
    â”‚        best_match = known_word
    â”‚
    â”œâ”€ Check Threshold
    â”‚  threshold = 2  # maximum allowed distance
    â”‚  
    â”‚  IF best_distance <= threshold:
    â”‚     word = best_match  # Replace with known word
    â”‚
    â””â”€ Replace in text
       text = text.replace(original_word, word)

Output: corrected_text
```

### C. Format Correction Algorithm

```
Input: text = "L 0 T E . 2 0 2"

â”œâ”€ LOT Format Correction
â”‚  pattern = r'LOT[EO]?'
â”‚  IF match(pattern, text):
â”‚     text = sub(pattern, 'LOTE', text)
â”‚
â”œâ”€ Date Format Normalization
â”‚  patterns = [
â”‚     r'\d{1,2}[\/-]\d{1,2}[\/-]\d{2,4}',
â”‚  ]
â”‚  (already in format, just validate)
â”‚
â”œâ”€ Alphanumeric Code Cleanup
â”‚  pattern = r'([A-Z]+)\s+(\d+)'
â”‚  text = sub(pattern, r'\1\2', text)  # Remove spaces
â”‚
â””â”€ Final Output Cleanup
   text = ' '.join(text.split())  # Normalize spaces

Output: formatted_text
```

---

## VIII. Fluxo de ConfiguraÃ§Ã£o (Config Loading)

```
Command: make ocr-compare

â”œâ”€ Makefile
â”‚  â””â”€ calls: python scripts/ocr/benchmark_ocrs.py
â”‚
â”œâ”€ benchmark_ocrs.py
â”‚  â”œâ”€ config = load_ocr_config('config/experiments/ocr_comparison.yaml')
â”‚  â”‚
â”‚  â””â”€ FOR each engine in config['engines']:
â”‚
â”œâ”€ OCREvaluator
â”‚  â”œâ”€ add_engine(engine_name, config_path)
â”‚  â”‚  â”œâ”€ engine_config = load_ocr_config(config_path)
â”‚  â”‚  â”œâ”€ engine_class = ENGINE_MAP[engine_name]
â”‚  â”‚  â””â”€ engine = engine_class(engine_config)
â”‚  â”‚
â”‚  â””â”€ evaluate_dataset(images_dir, ground_truth_file)
â”‚     â”œâ”€ FOR each image:
â”‚     â”‚  â”œâ”€ preprocessor = ImagePreprocessor(prep_config)
â”‚     â”‚  â”œâ”€ processed = preprocessor.process(image)
â”‚     â”‚  â”œâ”€ FOR each engine:
â”‚     â”‚  â”‚  â”œâ”€ text, conf = engine.extract_text(processed)
â”‚     â”‚  â”‚  â”œâ”€ Calculate metrics (CER, exact_match, etc.)
â”‚     â”‚  â”‚  â””â”€ Store result
â”‚     â”‚  â”‚
â”‚     â”‚  â””â”€ Save results.json
â”‚     â”‚
â”‚     â””â”€ Generate report
â”‚        â”œâ”€ comparison_summary.csv
â”‚        â”œâ”€ comparison_summary.png
â”‚        â””â”€ all_results.csv
â”‚
â””â”€ Output: outputs/ocr_benchmarks/comparison/
```

---

## IX. Performance Profiling

```
Para identificar gargalos:

1. Per-image timing breakdown
   â”œâ”€ Preprocessing: X ms
   â”œâ”€ Line detection (if enabled): Y ms
   â”œâ”€ OCR inference: Z ms
   â”œâ”€ Postprocessing: W ms
   â””â”€ Total: X+Y+Z+W ms

2. Memory profiling
   â”œâ”€ Peak RAM
   â”œâ”€ Peak VRAM
   â””â”€ Model size

3. Accuracy breakdown
   â”œâ”€ Exact match: XX%
   â”œâ”€ CER: XX%
   â””â”€ By confidence level

4. Bottleneck analysis
   â”œâ”€ If preprocessing dominant â†’ optimize prep
   â”œâ”€ If inference dominant â†’ use faster model
   â””â”€ If postprocessing dominant â†’ simplify rules
```

---

## X. Testing & Validation

```
Test Hierarchy:

â”œâ”€ Unit Tests
â”‚  â”œâ”€ Test each engine independently
â”‚  â”œâ”€ Test each normalizer
â”‚  â”œâ”€ Test postprocessor
â”‚  â””â”€ Test line detector
â”‚
â”œâ”€ Integration Tests
â”‚  â”œâ”€ Test preprocessing + engine + postprocessing
â”‚  â”œâ”€ Test with real images
â”‚  â””â”€ Compare with baselines
â”‚
â”œâ”€ End-to-End Tests
â”‚  â”œâ”€ Full pipeline with benchmark data
â”‚  â”œâ”€ Compare all engines
â”‚  â””â”€ Generate reports
â”‚
â””â”€ Performance Tests
   â”œâ”€ Benchmark speed (ms/image)
   â”œâ”€ Benchmark accuracy (CER, exact match)
   â””â”€ Benchmark resources (memory, GPU)
```

---

**Documento tÃ©cnico profundo criado! Agora vocÃª entende toda a arquitetura! ğŸ›ï¸**
