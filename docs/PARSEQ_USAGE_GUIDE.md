# ğŸ¯ Guia Completo: Escolhendo o Modelo PARSeq Ideal

## ğŸš¨ Problema Identificado

Seu dataset tem **texto multi-linha** (2+ linhas por imagem), e o modelo `parseq_tiny` estÃ¡ com **baixa acurÃ¡cia**.

## âœ… SoluÃ§Ã£o: Alternar entre Modelos

Agora vocÃª pode facilmente **testar e comparar** os 3 modelos PARSeq:

### ğŸ“Š Modelos DisponÃ­veis

| Modelo | Tamanho | Velocidade | Multi-linha | RecomendaÃ§Ã£o |
|--------|---------|------------|-------------|--------------|
| **TINY** | ~20MB | âš¡ Muito rÃ¡pida | âŒ Ruim | NÃ£o use! |
| **BASE** | ~60MB | âš¡ RÃ¡pida | âœ… Bom | â­ **USE ESTE!** |
| **LARGE** | ~100MB | ğŸ¢ MÃ©dia | âœ… Ã“timo | ğŸ† MÃ¡xima precisÃ£o |

---

## ğŸš€ Como Usar

### OpÃ§Ã£o 1: Testar Modelo Recomendado (BASE) ğŸ¯

```bash
make ocr-parseq-base
```

Isso vai:
- âœ… Usar modelo `parseq` (~60MB)
- âœ… Testar em todas as imagens do dataset
- âœ… Gerar relatÃ³rio em `outputs/ocr_benchmarks/parseq_base/`

---

### OpÃ§Ã£o 2: Comparar TODOS os Modelos ğŸ“Š

```bash
make ocr-parseq-compare
```

Isso vai:
1. âš¡ Testar **TINY** (rÃ¡pido, mas ruim multi-linha)
2. â­ Testar **BASE** (recomendado)
3. ğŸ† Testar **LARGE** (mÃ¡xima precisÃ£o)
4. ğŸ“Š **Gerar comparaÃ§Ã£o automÃ¡tica** mostrando qual Ã© melhor!

**Resultado esperado:**
```
ğŸ“Š RESULTADOS DA COMPARAÃ‡ÃƒO
================================================================================

Modelo               Exact Match     Conf. MÃ©dia     Tempo (ms)     CER
--------------------------------------------------------------------------------
TINY (~20MB)          35.42%          0.425           42.50          0.645
BASE (~60MB)          72.18%          0.782           58.30          0.278  â­
LARGE (~100MB)        85.94%          0.856           95.20          0.141  ğŸ†

================================================================================
ğŸ† RECOMENDAÃ‡Ã•ES
================================================================================
âœ… Melhor acurÃ¡cia: LARGE (~100MB) (85.94%)
â­ BASE: Melhor custo-benefÃ­cio
   - AcurÃ¡cia: 72.18%
   - Tempo: 58.30ms
```

---

### OpÃ§Ã£o 3: Testar MÃ¡xima PrecisÃ£o (LARGE) ğŸ†

```bash
make ocr-parseq-large
```

Use quando:
- âœ… Precisar da **melhor acurÃ¡cia possÃ­vel**
- âœ… Tiver **GPU potente**
- âš ï¸ NÃ£o se importar com tempo de processamento

---

### OpÃ§Ã£o 4: Apenas Analisar Resultados ğŸ“ˆ

Se vocÃª jÃ¡ rodou os testes, pode ver a comparaÃ§Ã£o sem rodar novamente:

```bash
make ocr-parseq-analyze
```

---

## ğŸ”§ Configurando o Modelo PadrÃ£o

Depois de testar, escolha o melhor modelo para seu caso:

### Para usar BASE (recomendado):

**Arquivo:** `config/ocr/parseq.yaml`
```yaml
model_name: 'parseq'  # BASE - melhor para multi-linha
```

Depois rode:
```bash
make ocr-parseq
```

---

### Para usar LARGE (mÃ¡xima precisÃ£o):

**Arquivo:** `config/ocr/parseq.yaml`
```yaml
model_name: 'parseq_patch16_224'  # LARGE - melhor precisÃ£o
```

Depois rode:
```bash
make ocr-parseq
```

---

## ğŸ“ Onde Ficam os Resultados?

Cada teste gera resultados em:

```
outputs/ocr_benchmarks/
â”œâ”€â”€ parseq_tiny/
â”‚   â””â”€â”€ parseq_results.json
â”œâ”€â”€ parseq_base/
â”‚   â””â”€â”€ parseq_results.json
â””â”€â”€ parseq_large/
    â””â”€â”€ parseq_results.json
```

---

## ğŸ¯ Comandos DisponÃ­veis

```bash
# Testes individuais
make ocr-parseq              # Usa modelo padrÃ£o (config/ocr/parseq.yaml)
make ocr-parseq-tiny         # Testa TINY (~20MB) âš ï¸ NÃ£o recomendado
make ocr-parseq-base         # Testa BASE (~60MB) â­ Recomendado
make ocr-parseq-large        # Testa LARGE (~100MB) ğŸ† MÃ¡xima precisÃ£o

# ComparaÃ§Ã£o e anÃ¡lise
make ocr-parseq-compare      # Testa TODOS + gera comparaÃ§Ã£o ğŸ“Š
make ocr-parseq-analyze      # Apenas analisa resultados existentes ğŸ“ˆ

# Setup
make ocr-parseq-setup        # Baixa TODOS os modelos de uma vez
make ocr-parseq-validate     # Valida implementaÃ§Ã£o
```

---

## ğŸ’¡ RecomendaÃ§Ã£o Final

**Para seu dataset (multi-linha):**

1. âœ… **Primeiro passo:** Rode `make ocr-parseq-compare`
2. âœ… **Veja os resultados** e escolha entre BASE ou LARGE
3. âœ… **Configure** o modelo escolhido em `config/ocr/parseq.yaml`
4. âœ… **Use** `make ocr-parseq` no pipeline

**Provavelmente BASE serÃ¡ suficiente** (70-85% acurÃ¡cia), mas se precisar de mais, use LARGE (80-95%).

**NUNCA use TINY para multi-linha!** âŒ

---

## ğŸ› Troubleshooting

### Modelo nÃ£o carrega
```bash
# Limpar cache
rm -rf ~/.cache/torch/hub/baudm_parseq_*

# Recarregar
make ocr-parseq-setup
```

### CUDA out of memory
```yaml
# Em config/ocr/parseq_*.yaml
device: 'cpu'  # Mudar de cuda para cpu
```

### Resultados nÃ£o aparecem
```bash
# Verificar se arquivos existem
ls -la outputs/ocr_benchmarks/parseq_*/

# Rodar anÃ¡lise manualmente
python scripts/ocr/compare_parseq_models.py
```

---

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **ImplementaÃ§Ã£o**: `docs/PARSEQ_IMPLEMENTATION.md`
- **Quickstart**: `docs/PARSEQ_QUICKSTART.md`
- **SeleÃ§Ã£o de Modelo**: `docs/PARSEQ_MODEL_SELECTION.md`
- **Multi-modelo**: `docs/PARSEQ_MULTIMODEL_UPDATE.md`

---

**âœ… Tudo pronto! Agora vocÃª pode escolher o melhor modelo PARSeq para seu dataset multi-linha!** ğŸ‰
