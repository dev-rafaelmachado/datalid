"""
üî§ PARSeq Engine (Permutation Auto-regressive Sequence)
Wrapper para PARSeq OCR usando a vers√£o TINE (Tiny Efficient).
Desenvolvido por Darwin Bautista (baudm/parseq)
"""

from typing import Any, Dict, Tuple

import cv2
import numpy as np
import torch
from loguru import logger

from src.ocr.engines.base import OCREngineBase


class PARSeqEngine(OCREngineBase):
    """
    Engine para PARSeq OCR - vers√£o TINE (Tiny Efficient).
    
    PARSeq √© um modelo Transformer-based para OCR em cena que usa
    auto-regress√£o permutacional para maior efici√™ncia e precis√£o.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa PARSeq Engine.
        
        Args:
            config: Dicion√°rio de configura√ß√£o com as seguintes op√ß√µes:
                - model_name: Nome do modelo ('parseq-tiny', 'parseq', 'parseq-large')
                - device: Device para infer√™ncia ('cuda' ou 'cpu')
                - img_height: Altura da imagem de entrada (padr√£o: 32)
                - img_width: Largura da imagem de entrada (padr√£o: 128)
                - max_length: Comprimento m√°ximo de sequ√™ncia (padr√£o: 25)
                - batch_size: Tamanho do batch (padr√£o: 1)
        """
        super().__init__(config)
        self.model_name = config.get('model_name', 'parseq_tiny')
        self.device = config.get('device', 'cuda')
        self.img_height = config.get('img_height', 32)
        self.img_width = config.get('img_width', 128)
        self.max_length = config.get('max_length', 25)
        self.batch_size = config.get('batch_size', 1)
        
        self.model = None
        self.transform = None
        self.img_transform = None
    
    def initialize(self) -> None:
        """
        Inicializa o PARSeq (vers√£o TINE).
        
        Carrega o modelo via torch.hub do reposit√≥rio oficial baudm/parseq.
        A vers√£o TINE (parseq-tiny) √© otimizada para infer√™ncia r√°pida.
        """
        if self._is_initialized:
            return
        
        try:
            from PIL import Image
            from torchvision import transforms

            # Mapear nomes de modelos
            model_map = {
                'parseq_tiny': 'parseq_tiny',
                'parseq-tiny': 'parseq_tiny',
                'tiny': 'parseq_tiny',
                'parseq': 'parseq',
                'parseq-base': 'parseq',
                'base': 'parseq',
                'parseq_patch16_224': 'parseq_patch16_224',
                'parseq-patch16-224': 'parseq_patch16_224',
                'parseq-large': 'parseq_patch16_224',
                'large': 'parseq_patch16_224'
            }
            
            # Normalizar nome do modelo
            model_name = model_map.get(self.model_name.lower(), self.model_name)
            
            model_info = {
                'parseq_tiny': '~20MB, r√°pido, boa precis√£o',
                'parseq': '~60MB, balanceado, muito boa precis√£o',
                'parseq_patch16_224': '~100MB, mais lento, excelente precis√£o'
            }
            
            logger.info(f"üîÑ Inicializando PARSeq ({model_name})...")
            logger.info(f"   Caracter√≠sticas: {model_info.get(model_name, 'modelo personalizado')}")
            
            # Verificar dispositivo
            if self.device == 'cuda' and not torch.cuda.is_available():
                logger.warning("‚ö†Ô∏è CUDA n√£o dispon√≠vel, usando CPU")
                self.device = 'cpu'
            
            # Carregar modelo do torch hub (baudm/parseq)
            try:
                logger.info(f"üì• Baixando modelo {model_name} via torch.hub...")
                self.model = torch.hub.load(
                    'baudm/parseq',
                    model_name,
                    pretrained=True,
                    trust_repo=True,
                    verbose=False
                )
                self.model.to(self.device)
                self.model.eval()
                logger.info(f"‚úÖ Modelo {model_name} carregado com sucesso!")
                
                # Atualizar nome do modelo
                self.model_name = model_name
                
            except Exception as e:
                logger.error(f"‚ùå Falha ao carregar via torch.hub: {e}")
                logger.error("Verifique sua conex√£o com a internet e tente novamente.")
                logger.info("üí° Alternativa: clone manualmente o reposit√≥rio baudm/parseq")
                raise ImportError(
                    f"PARSeq n√£o p√¥de ser carregado: {e}\n"
                    "Solu√ß√µes:\n"
                    "1. Verifique conex√£o com internet\n"
                    "2. Execute: git clone https://github.com/baudm/parseq.git\n"
                    "3. Instale depend√™ncias: pip install torch torchvision Pillow"
                )
            
            # Definir transforma√ß√µes de imagem
            # PARSeq espera imagens normalizadas com ImageNet stats
            self.img_transform = transforms.Compose([
                transforms.Resize((self.img_height, self.img_width), 
                                 interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            self.engine = self.model
            
            logger.info(f"‚úÖ PARSeq inicializado com sucesso!")
            logger.info(f"   Modelo: {self.model_name}")
            logger.info(f"   Device: {self.device}")
            logger.info(f"   Input size: {self.img_height}x{self.img_width}")
            self._is_initialized = True
            
        except ImportError as e:
            logger.error(f"‚ùå Erro ao importar depend√™ncias: {e}")
            logger.error("Execute: pip install torch torchvision Pillow")
            raise
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar PARSeq: {e}")
            raise
    
    def extract_text(self, image: np.ndarray) -> Tuple[str, float]:
        """
        Extrai texto usando PARSeq TINE.
        
        Args:
            image: Imagem numpy array (RGB ou BGR)
            
        Returns:
            Tupla (texto, confian√ßa)
        """
        if not self._is_initialized:
            self.initialize()
        
        if not self.validate_image(image):
            return "", 0.0
        
        try:
            from PIL import Image

            # Converter BGR para RGB se necess√°rio
            if len(image.shape) == 3 and image.shape[2] == 3:
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            elif len(image.shape) == 2:
                image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
            else:
                image_rgb = image
            
            # Converter para PIL Image
            pil_image = Image.fromarray(image_rgb)
            
            # Aplicar transforma√ß√µes (resize + normaliza√ß√£o)
            image_tensor = self.img_transform(pil_image).unsqueeze(0).to(self.device)
            
            # Infer√™ncia com PARSeq
            with torch.no_grad():
                # O modelo PARSeq retorna logits de forma [batch, seq_len, vocab_size]
                logits = self.model(image_tensor)
                
                # Calcular probabilidades
                probs = logits.softmax(-1)
                
                # Calcular confian√ßa m√©dia
                max_probs = probs.max(-1)[0]  # M√°xima probabilidade por posi√ß√£o
                
                # Filtrar posi√ß√µes com confian√ßa muito baixa (provavelmente padding)
                valid_mask = max_probs > 0.01
                if valid_mask.any():
                    avg_confidence = max_probs[valid_mask].mean().item()
                else:
                    avg_confidence = 0.0
                
                # Decodificar usando tokenizer
                # O tokenizer.decode() espera o tensor de probabilidades ou logits
                if hasattr(self.model, 'tokenizer'):
                    try:
                        # tokenizer.decode espera tensor de distribui√ß√µes
                        # Passa os logits diretamente (ele far√° softmax internamente se necess√°rio)
                        decoded_result = self.model.tokenizer.decode(logits)
                        
                        # O resultado √© uma tupla: (texto_list, confidencias_tensor)
                        logger.debug(f"üì¶ decoded_result type: {type(decoded_result)}")
                        logger.debug(f"üì¶ decoded_result: {decoded_result}")
                        
                        if isinstance(decoded_result, tuple) and len(decoded_result) >= 1:
                            text_list = decoded_result[0]
                            logger.debug(f"üìù text_list type: {type(text_list)}, value: {text_list}")
                            if isinstance(text_list, list) and len(text_list) > 0:
                                text = text_list[0]
                                logger.debug(f"‚úÖ Extracted text from list: '{text}'")
                            else:
                                text = str(text_list) if text_list else ""
                                logger.debug(f"‚ö†Ô∏è  text_list not list or empty, using: '{text}'")
                        elif isinstance(decoded_result, list) and len(decoded_result) > 0:
                            text = decoded_result[0]
                            logger.debug(f"‚úÖ Extracted from list directly: '{text}'")
                        elif isinstance(decoded_result, str):
                            text = decoded_result
                            logger.debug(f"‚úÖ decoded_result is string: '{text}'")
                        else:
                            text = str(decoded_result) if decoded_result else ""
                            logger.debug(f"‚ö†Ô∏è  Fallback str conversion: '{text}'")
                            
                    except Exception as e:
                        logger.debug(f"Erro na decodifica√ß√£o com tokenizer: {e}")
                        
                        # Fallback: decodifica√ß√£o manual
                        # EOS=0, ent√£o paramos quando encontramos
                        pred_indices = probs.argmax(-1).squeeze(0).cpu().tolist()
                        
                        # Filtrar tokens especiais
                        eos_id = 0
                        bos_id = 95 if hasattr(self.model, 'bos_id') and self.model.bos_id == 95 else 95
                        pad_id = 96 if hasattr(self.model, 'pad_id') and self.model.pad_id == 96 else 96
                        
                        filtered_indices = []
                        for idx in pred_indices:
                            if idx == eos_id:  # Parar no EOS
                                break
                            if idx not in [eos_id, bos_id, pad_id]:
                                filtered_indices.append(idx)
                        
                        # Tentar mapear para caracteres ASCII (charset padr√£o do PARSeq)
                        # O PARSeq geralmente usa: 0-9, a-z, A-Z, e alguns s√≠mbolos
                        # √çndices come√ßam ap√≥s tokens especiais
                        chars = []
                        for idx in filtered_indices:
                            # Mapear √≠ndices para caracteres
                            # PARSeq tipicamente usa este charset:
                            # 0=EOS, 95=BOS, 96=PAD, e os outros s√£o caracteres
                            if 1 <= idx <= 94:
                                # Tentar mapear para caractere
                                # Este √© um fallback aproximado baseado no charset comum
                                charset = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ "
                                if idx - 1 < len(charset):
                                    chars.append(charset[idx - 1])
                        
                        text = ''.join(chars)
                else:
                    logger.warning("‚ö†Ô∏è Modelo n√£o tem tokenizer")
                    text = ""
            
            # P√≥s-processar texto
            logger.debug(f"üìù Texto ANTES do postprocess: '{text}'")
            text = self.postprocess(text)
            logger.debug(f"üìù Texto AP√ìS postprocess: '{text}'")
            text = text.strip()
            logger.debug(f"üìù Texto AP√ìS strip: '{text}'")
            
            logger.debug(f"üìù PARSeq TINE: '{text}' (conf: {avg_confidence:.3f})")
            
            return text, avg_confidence
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair texto com PARSeq: {e}")
            logger.debug(f"Stack trace: {type(e).__name__}: {str(e)}", exc_info=True)
            return "", 0.0
    
    def get_name(self) -> str:
        """Retorna nome do engine."""
        return "parseq"
    
    def get_version(self) -> str:
        """Retorna vers√£o do PARSeq."""
        try:
            import torch
            return f"torch-{torch.__version__}"
        except:
            return "unknown"


__all__ = ['PARSeqEngine']
