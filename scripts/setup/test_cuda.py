"""
üß™ Teste de Disponibilidade CUDA e GPU
Verifica se PyTorch consegue acessar a GPU.
"""

import platform
import torch
import sys
from pathlib import Path

# Adicionar root ao PYTHONPATH
ROOT = Path(__file__).resolve().parent.parent
sys.path.append(str(ROOT))


def print_header():
    """Imprime cabe√ßalho."""
    print("\n" + "="*60)
    print("üß™ TESTE DE CUDA E GPU")
    print("="*60)


def test_cuda():
    """Testa disponibilidade do CUDA."""
    print("\nüìã INFORMA√á√ïES DO SISTEMA:")
    print(f"  ‚Ä¢ Sistema Operacional: {platform.system()} {platform.release()}")
    print(f"  ‚Ä¢ Python: {sys.version.split()[0]}")
    print(f"  ‚Ä¢ PyTorch: {torch.__version__}")

    print("\nüîç CUDA:")
    cuda_available = torch.cuda.is_available()
    print(f"  ‚Ä¢ CUDA Dispon√≠vel: {'‚úÖ SIM' if cuda_available else '‚ùå N√ÉO'}")

    if cuda_available:
        print(f"  ‚Ä¢ Vers√£o CUDA: {torch.version.cuda}")
        print(f"  ‚Ä¢ cuDNN: {torch.backends.cudnn.version()}")

        print("\nüéÆ GPUs DETECTADAS:")
        num_gpus = torch.cuda.device_count()
        print(f"  ‚Ä¢ Quantidade: {num_gpus}")

        for i in range(num_gpus):
            props = torch.cuda.get_device_properties(i)
            print(f"\n  üì± GPU {i}: {props.name}")
            print(
                f"     ‚Ä¢ Mem√≥ria Total: {props.total_memory / 1024**3:.2f} GB")
            print(f"     ‚Ä¢ Compute Capability: {props.major}.{props.minor}")

            # Teste de aloca√ß√£o
            try:
                print(f"\n  ‚ö° Testando aloca√ß√£o na GPU {i}...")
                device = torch.device(f'cuda:{i}')
                x = torch.randn(1000, 1000, device=device)
                y = torch.randn(1000, 1000, device=device)
                z = x @ y
                print(f"     ‚úÖ Teste de multiplica√ß√£o de matrizes OK!")

                # Mem√≥ria usada
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                print(f"     ‚Ä¢ Mem√≥ria Alocada: {allocated:.2f} GB")
                print(f"     ‚Ä¢ Mem√≥ria Reservada: {cached:.2f} GB")

                # Limpar
                del x, y, z
                torch.cuda.empty_cache()

            except Exception as e:
                print(f"     ‚ùå Erro no teste: {str(e)}")

        return True
    else:
        print("\n‚ùå CUDA N√ÉO DISPON√çVEL")
        print("\nüí° POSS√çVEIS CAUSAS:")
        print("  1. Driver NVIDIA n√£o instalado")
        print("  2. PyTorch instalado sem suporte CUDA")
        print("  3. GPU n√£o compat√≠vel")
        print("\nüîß SOLU√á√ÉO:")
        print("  ‚Ä¢ Instale o driver NVIDIA: https://www.nvidia.com/drivers")
        print("  ‚Ä¢ Reinstale PyTorch com CUDA:")
        print("    pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")

        return False


def test_yolo_gpu():
    """Testa se YOLO consegue usar GPU."""
    print("\nüéØ TESTE YOLO + GPU:")
    try:
        from ultralytics import YOLO

        print("  ‚Ä¢ Ultralytics instalado: ‚úÖ")

        if torch.cuda.is_available():
            print("  ‚Ä¢ Tentando carregar modelo...")
            model = YOLO("yolov8n.pt")
            print("  ‚Ä¢ Modelo carregado: ‚úÖ")

            # Verificar device
            device = next(model.model.parameters()).device
            print(f"  ‚Ä¢ Device padr√£o: {device}")

            print("\n‚úÖ YOLO pronto para usar GPU!")
        else:
            print("  ‚ö†Ô∏è  YOLO funcionar√° apenas com CPU (lento)")

    except ImportError:
        print("  ‚ùå Ultralytics n√£o instalado")
        print("  üí° Instale: pip install ultralytics")
    except Exception as e:
        print(f"  ‚ùå Erro: {str(e)}")


def print_recommendations():
    """Imprime recomenda√ß√µes."""
    print("\n" + "="*60)
    print("üí° RECOMENDA√á√ïES PARA TREINAMENTO:")
    print("="*60)

    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3

        print(f"\n‚úÖ GPU Detectada: {gpu_name}")
        print(f"‚úÖ Mem√≥ria: {total_mem:.1f}GB")

        # Recomenda√ß√µes baseadas na GPU
        if "1660" in gpu_name or total_mem < 8:
            print("\nüìä Configura√ß√µes Recomendadas:")
            print("  ‚Ä¢ Modelo: YOLOv8n ou YOLOv8s")
            print("  ‚Ä¢ Batch Size: 16")
            print("  ‚Ä¢ Image Size: 640")
            print("  ‚Ä¢ Workers: 4")
        elif total_mem >= 8:
            print("\nüìä Configura√ß√µes Recomendadas:")
            print("  ‚Ä¢ Modelo: YOLOv8s ou YOLOv8m")
            print("  ‚Ä¢ Batch Size: 32")
            print("  ‚Ä¢ Image Size: 640")
            print("  ‚Ä¢ Workers: 8")

        print("\nüöÄ Pronto para treinar!")
        print("   Execute: make train-test")
    else:
        print("\n‚ùå Sem GPU - Treinamento ser√° MUITO lento")
        print("üí° Considere usar Google Colab ou Kaggle")


def main():
    """Executa teste completo."""
    print_header()

    cuda_ok = test_cuda()
    test_yolo_gpu()
    print_recommendations()

    print("\n" + "="*60)

    if cuda_ok:
        print("‚úÖ Sistema pronto para treinamento!")
        sys.exit(0)
    else:
        print("‚ö†Ô∏è  Configure CUDA antes de treinar")
        sys.exit(1)


if __name__ == "__main__":
    main()
