"""
üìä Compara√ß√£o de Modelos PARSeq
Compara os resultados de tiny, base e large para determinar o melhor modelo
"""

import json
import sys
from pathlib import Path
from typing import Dict, List

import pandas as pd
from loguru import logger

# Configurar logger
logger.remove()
logger.add(sys.stdout, format="<level>{message}</level>", level="INFO")


def load_results(output_dir: str) -> Dict:
    """Carrega resultados de um modelo."""
    results_file = Path(output_dir) / "parseq_results.json"
    
    if not results_file.exists():
        logger.warning(f"‚ö†Ô∏è  Arquivo n√£o encontrado: {results_file}")
        return None
    
    with open(results_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def calculate_metrics(results: List[Dict]) -> Dict:
    """Calcula m√©tricas agregadas."""
    if not results:
        return {
            'exact_match': 0.0,
            'partial_match': 0.0,
            'avg_confidence': 0.0,
            'avg_time': 0.0,
            'char_error_rate': 1.0,
            'total_images': 0
        }
    
    total = len(results)
    
    exact_matches = sum(1 for r in results if r.get('exact_match', 0) > 0.99)
    partial_matches = sum(1 for r in results if r.get('partial_match', 0) > 0.5)
    avg_confidence = sum(r.get('confidence', 0) for r in results) / total if total > 0 else 0
    avg_time = sum(r.get('processing_time', 0) for r in results) / total if total > 0 else 0
    avg_cer = sum(r.get('character_error_rate', 1.0) for r in results) / total if total > 0 else 1.0
    
    return {
        'exact_match': (exact_matches / total) * 100 if total > 0 else 0,
        'partial_match': (partial_matches / total) * 100 if total > 0 else 0,
        'avg_confidence': avg_confidence,
        'avg_time': avg_time * 1000,  # Converter para ms
        'char_error_rate': avg_cer,
        'total_images': total
    }


def compare_models():
    """Compara os tr√™s modelos PARSeq."""
    logger.info("=" * 80)
    logger.info("üìä COMPARA√á√ÉO DE MODELOS PARSEQ")
    logger.info("=" * 80)
    logger.info("")
    
    models = {
        'TINY (~20MB)': 'outputs/ocr_benchmarks/parseq_tiny',
        'BASE (~60MB)': 'outputs/ocr_benchmarks/parseq_base',
        'LARGE (~100MB)': 'outputs/ocr_benchmarks/parseq_large'
    }
    
    all_metrics = {}
    
    # Carregar resultados de cada modelo
    for model_name, output_dir in models.items():
        logger.info(f"üì¶ Carregando {model_name}...")
        results = load_results(output_dir)
        
        if results:
            metrics = calculate_metrics(results)
            all_metrics[model_name] = metrics
            logger.info(f"   ‚úÖ {metrics['total_images']} imagens processadas")
        else:
            logger.warning(f"   ‚ö†Ô∏è  N√£o encontrado - rode 'make ocr-parseq-compare' primeiro")
    
    if not all_metrics:
        logger.error("‚ùå Nenhum resultado encontrado!")
        logger.info("üí° Execute: make ocr-parseq-compare")
        return
    
    # Criar tabela de compara√ß√£o
    logger.info("")
    logger.info("=" * 80)
    logger.info("üìä RESULTADOS DA COMPARA√á√ÉO")
    logger.info("=" * 80)
    logger.info("")
    
    # Cabe√ßalho
    print(f"{'Modelo':<20} {'Exact Match':<15} {'Conf. M√©dia':<15} {'Tempo (ms)':<15} {'CER':<10}")
    print("-" * 80)
    
    # Dados
    best_accuracy = None
    best_model = None
    
    for model_name, metrics in all_metrics.items():
        exact = metrics['exact_match']
        conf = metrics['avg_confidence']
        time = metrics['avg_time']
        cer = metrics['char_error_rate']
        
        # Marcar melhor acur√°cia
        marker = ""
        if best_accuracy is None or exact > best_accuracy:
            best_accuracy = exact
            best_model = model_name
        
        print(f"{model_name:<20} {exact:>6.2f}% {' '*7} {conf:>6.3f} {' '*7} {time:>6.2f} {' '*7} {cer:>6.3f}")
    
    print("-" * 80)
    
    # Resumo
    logger.info("")
    logger.info("=" * 80)
    logger.info("üèÜ RECOMENDA√á√ïES")
    logger.info("=" * 80)
    
    if best_model:
        logger.info(f"‚úÖ Melhor acur√°cia: {best_model} ({best_accuracy:.2f}%)")
    
    # Verificar se TINY foi usado
    if 'TINY (~20MB)' in all_metrics:
        tiny_metrics = all_metrics['TINY (~20MB)']
        if tiny_metrics['exact_match'] < 50:
            logger.warning("")
            logger.warning("‚ö†Ô∏è  ATEN√á√ÉO: TINY tem baixa acur√°cia!")
            logger.warning("   Para dataset multi-linha, use BASE ou LARGE")
    
    # Verificar se BASE foi usado
    if 'BASE (~60MB)' in all_metrics:
        base_metrics = all_metrics['BASE (~60MB)']
        logger.info("")
        logger.info(f"‚≠ê BASE: Melhor custo-benef√≠cio")
        logger.info(f"   - Acur√°cia: {base_metrics['exact_match']:.2f}%")
        logger.info(f"   - Tempo: {base_metrics['avg_time']:.2f}ms")
    
    # Verificar se LARGE foi usado
    if 'LARGE (~100MB)' in all_metrics:
        large_metrics = all_metrics['LARGE (~100MB)']
        logger.info("")
        logger.info(f"üèÜ LARGE: M√°xima precis√£o")
        logger.info(f"   - Acur√°cia: {large_metrics['exact_match']:.2f}%")
        logger.info(f"   - Tempo: {large_metrics['avg_time']:.2f}ms")
    
    logger.info("")
    logger.info("=" * 80)
    logger.info("üí° PR√ìXIMOS PASSOS")
    logger.info("=" * 80)
    logger.info("")
    
    if best_model == 'TINY (~20MB)':
        logger.warning("‚ö†Ô∏è  TINY √© o melhor? Isso √© incomum para multi-linha!")
        logger.info("   Verifique se o dataset realmente tem multi-linha")
        logger.info("   ou rode novamente: make ocr-parseq-compare")
    elif best_model == 'BASE (~60MB)':
        logger.info("‚úÖ Use BASE como padr√£o:")
        logger.info("   1. Edite config/ocr/parseq.yaml ‚Üí model_name: 'parseq'")
        logger.info("   2. Rode: make ocr-parseq")
    elif best_model == 'LARGE (~100MB)':
        logger.info("üèÜ Para m√°xima precis√£o, use LARGE:")
        logger.info("   1. Edite config/ocr/parseq.yaml ‚Üí model_name: 'parseq_patch16_224'")
        logger.info("   2. Rode: make ocr-parseq")
    
    logger.info("")
    logger.info("=" * 80)


if __name__ == "__main__":
    compare_models()
